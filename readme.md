#🚀 Overview
This project implements a modular data processing pipeline using PySpark to ingest, clean, enrich, analyze, and store structured server log data. It is designed to handle large volumes of logs generated by web-based services, providing insights into system performance, user behavior, and operational issues.
#

#🎯 Objectives
- Ingest raw server logs and user metadata.
- Clean and preprocess the data to handle nulls and inconsistencies.
#

#🗂️ Project Structure
- dataset/: Contains 2 folder raw and cleaned. Raw contains sample datasets for server logs and user metadata. Cleaned contains dataset which was my outcomes after applying operation.
- notebooks/: Jupyter notebooks for exploratory data analysis and pipeline development.

#🔄 Data Processing Pipeline
- **Data Cleaning Strategy**
    - Implemented selective null handling:
      1. Dropped rows with null records in columns like timestamp, user_id, ip_address, request_id.
      2. Replaced records with nulls in columns like log_level, endpoints, status_code, response_time using sql function like mean, mode, average, most used, etc.
    - Dropping rows based on assumption:
      1. Dropped the rows with status_code less than 150 as less than 15% efficient.
- **Data Enrichment**
    - Implemented broadcast joins for efficient data merging.
    - Created column like hour of the day, day of the week, is error, good status, good response based on respected column needed and function needed.
#


# 📊 Key Findings
- Most used log level is **ERROR**.
- Most used endpoint is **/api/delete/**
- Mean response time is **2.523**.
- Average status code is **390**.
- Remaining finding is in the repo.
#

Built with ❤️ using PySpark by [Sanket Kabra](https://github.com/sanketdv)
